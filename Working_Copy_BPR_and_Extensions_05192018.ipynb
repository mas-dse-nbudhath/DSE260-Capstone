{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import defaultdict\n",
    "import struct\n",
    "import numpy as np\n",
    "import numpy\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "def format_time(t):\n",
    "    return t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#this function helps to visualize the dict\n",
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "# take(1, prod_desc.values())\n",
    "\n",
    "# load data after creating features\n",
    "def load_data_hybrid(data_path, min_items=2, min_users=2, sampling= True, sample_size = 0.5):\n",
    "    user_ratings = defaultdict(set)\n",
    "    item_ratings = defaultdict(set)\n",
    "    max_u_id = -1\n",
    "    max_i_id = -1\n",
    "    user_count = 0\n",
    "    item_count = 0\n",
    "    reviews = 0\n",
    "    users = {}  # aid to id LUT\n",
    "    items = {}  # asid to id LUT\n",
    "    records = {} # all records\n",
    "    features = {}\n",
    "    random.seed(0)\n",
    "    columns = None\n",
    "    offset_to_features = 3\n",
    "    with open(data_path, 'r') as f:\n",
    "        bad_actor = 0\n",
    "        for line in f.readlines():\n",
    "            record = {}\n",
    "            split_line = line.split(\",\")\n",
    "            if columns is None:\n",
    "                columns = [e.rstrip() for e in split_line]\n",
    "                continue\n",
    "            #if (sampling and random.random()>sample_size):\n",
    "            #    continue\n",
    "            reviews += 1\n",
    "            \n",
    "            if (len(split_line) > len(columns)):\n",
    "                bad_actor = bad_actor + 1\n",
    "                continue\n",
    "            else:\n",
    "                auid, asid, _ = split_line[0:offset_to_features]\n",
    "                record = {columns[i]:split_line[i].rstrip() for i in  range (offset_to_features, len(split_line))}\n",
    "\n",
    "            u, i = None, None\n",
    "\n",
    "            if auid in users:\n",
    "                u = users[auid]\n",
    "            else:\n",
    "                user_count += 1  # new user so increment\n",
    "                users[auid] = user_count\n",
    "                u = user_count\n",
    "            \n",
    "            if asid in items:\n",
    "                i = items[asid]\n",
    "            else:\n",
    "                item_count += 1  # new i so increment\n",
    "                items[asid] = item_count\n",
    "                i = item_count\n",
    "                \n",
    "                for c in ['price_delta_calc1','price_delta_calc2','price_delta_l4avg']:\n",
    "                    if c in record:\n",
    "                        record[c] = float(record[c])\n",
    "                if 'price' in record:\n",
    "                    if record['price'] == '':\n",
    "                        record['price'] = 0\n",
    "                    else:\n",
    "                        record['price'] = float(record['price'])\n",
    "                if 'polarity' in record:\n",
    "                    record['polarity']= round((float(record['polarity'])),2)\n",
    "                    \n",
    "                if 'feature_vector' in record:\n",
    "                    if len(record['feature_vector']) == 0:\n",
    "                        record['feature_vector'] = list(np.zeros(4524))\n",
    "                    else:\n",
    "                        record['feature_vector'] = [int(el) for el in list(record['feature_vector'])[:-1][1:]]\n",
    "    \n",
    "                for c in ['top_categories','rating','percentile_hotcoded','season','level4','sentiment']:\n",
    "                    if c in record:\n",
    "                        record[c] = [int(el) for el in list(record[c])[:-2][1:]]\n",
    "                records[i] = record\n",
    "            \n",
    "            user_ratings[u].add(i)\n",
    "            item_ratings[i].add(u)\n",
    "            max_u_id = max(u, max_u_id)\n",
    "            max_i_id = max(i, max_i_id)\n",
    "            \n",
    "    print (\"max_u_id: \", max_u_id)\n",
    "    print (\"max_i_id: \", max_i_id)\n",
    "    print (\"reviews : \", reviews)\n",
    "\n",
    "\n",
    "    # filter out users w/ less than X reviews\n",
    "    num_u_id = 0\n",
    "    num_i_id = 0\n",
    "    num_reviews = 0\n",
    "    user_ratings_filtered = defaultdict(set)\n",
    "    for u, ids in user_ratings.items():\n",
    "        if len(ids) > min_items:\n",
    "            user_ratings_filtered[u] = ids\n",
    "            num_u_id += 1\n",
    "            num_reviews += len(ids)\n",
    "            \n",
    "    item_ratings_filtered = defaultdict(set)\n",
    "    for ids, u in item_ratings.items():\n",
    "        if len(u) > min_users:\n",
    "            # keep\n",
    "            item_ratings_filtered[ids] = u\n",
    "            num_i_id += 1\n",
    "    \n",
    "    feature_keys = records[1].keys() #should be same as columns[offset:]\n",
    "    features = {k:{i:records[i][k] for i in range(1,len(records)+1)} for k in feature_keys}\n",
    "\n",
    "    print (\"u_id: \", num_u_id)\n",
    "    print (\"i_id: \", num_i_id)\n",
    "    print (\"reviews : \", num_reviews)\n",
    "    #return max_u_id, max_i_id, users, items, user_ratings_filtered,\\\n",
    "    #            item_ratings_filtered, brands, prices, prod_desc, prod_cat,price_feature,season_feature\n",
    "    return max_u_id, max_i_id, users, items, user_ratings_filtered,item_ratings_filtered, features\n",
    "\n",
    "#load image features for the given asin collection into dictionary\n",
    "def load_image_features(path, items):\n",
    "    count=0\n",
    "    image_features = {}\n",
    "    f = open(path, 'rb')\n",
    "    while True:\n",
    "        asin = f.read(10)\n",
    "        if asin == '': break\n",
    "        features_bytes = f.read(16384) # 4 * 4096 = 16KB, fast read, don't unpack\n",
    "  \n",
    "        if asin in items: #only unpack 4096 bytes if w need it -- big speed up\n",
    "            features = (np.fromstring(features_bytes, dtype=np.float32)/58.388599)\n",
    "            iid=items[asin]\n",
    "            if len(features)==0:\n",
    "                image_features[iid] = np.zeros(4096)\n",
    "            else:\n",
    "                image_features[iid] = features\n",
    "    \n",
    "    return image_features\n",
    "\n",
    "def uniform_sample_batch(train_ratings, test_ratings, item_count, advanced_features):\n",
    "    neg_items = 2\n",
    "    for u in train_ratings.keys():\n",
    "        t = []\n",
    "        iv = []\n",
    "        jv = []\n",
    "        for i in train_ratings[u]:\n",
    "            if (u in test_ratings.keys()):\n",
    "                if (i != test_ratings[u]):  # make sure it's not in the test set\n",
    "                    for k in range(1,neg_items):\n",
    "                        j = random.randint(1, item_count)\n",
    "                        while j in train_ratings[u]:\n",
    "                            j = random.randint(1, item_count)\n",
    "                        # sometimes there will not be an image for given product\n",
    "                        try:\n",
    "                            advanced_features[i]\n",
    "                            advanced_features[j]\n",
    "                        except KeyError:\n",
    "                            continue\n",
    "                        iv.append(advanced_features[i])\n",
    "                        jv.append(advanced_features[j])\n",
    "                        t.append([u, i, j])\n",
    "            else:\n",
    "                for k in range(1,neg_items):\n",
    "                    j = random.randint(1, item_count)\n",
    "                    while j in train_ratings[u]:\n",
    "                        j = random.randint(1, item_count)\n",
    "                    # sometimes there will not be an image for given product\n",
    "                    try:\n",
    "                        advanced_features[i]\n",
    "                        advanced_features[j]\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                    iv.append(advanced_features[i])\n",
    "                    jv.append(advanced_features[j])\n",
    "                    t.append([u, i, j])\n",
    "\n",
    "        # block if queue is full\n",
    "        if len(iv)>1:\n",
    "            yield numpy.asarray(t), numpy.vstack(tuple(iv)), numpy.vstack(tuple(jv))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "def test_batch_generator_by_user(train_ratings, test_ratings, item_ratings, item_count, advanced_features, cold_start = False, cold_start_thresh = 5):\n",
    "    # using leave one cv\n",
    "    for u in random.sample(test_ratings.keys(), 4000):\n",
    "    #for u in test_ratings.keys():\n",
    "        i = test_ratings[u]\n",
    "        if (cold_start and len(item_ratings[i]) > cold_start_thresh-1):\n",
    "            continue\n",
    "        t = []\n",
    "        ilist = []\n",
    "        jlist = []\n",
    "        count = 0\n",
    "        for j in random.sample(range(item_count), 100):\n",
    "            # find item not in test[u] and train[u]\n",
    "            if j != test_ratings[u] and not (j in train_ratings[u]):\n",
    "                try:\n",
    "                    advanced_features[i]\n",
    "                    advanced_features[j]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "                count += 1\n",
    "                t.append([u, i, j])\n",
    "                ilist.append(advanced_features[i])\n",
    "                jlist.append(advanced_features[j])\n",
    "\n",
    "        # print numpy.asarray(t).shape\n",
    "        # print numpy.vstack(tuple(ilist)).shape\n",
    "        # print numpy.vstack(tuple(jlist)).shape\n",
    "        if (len(ilist) == 0):\n",
    "            #print \"could not find neg item for user, count: \", count, u\n",
    "            continue\n",
    "        yield numpy.asarray(t), numpy.vstack(tuple(ilist)), numpy.vstack(tuple(jlist))\n",
    "\n",
    "def generate_test(user_ratings):\n",
    "    '''\n",
    "    for each user, random select one rating into test set\n",
    "    '''\n",
    "    user_test = dict()\n",
    "    for u, i_list in user_ratings.items():\n",
    "        user_test[u] = random.sample(user_ratings[u], 1)[0]\n",
    "    return user_test\n",
    "\n",
    "#user_count, item_count, users, items, user_ratings, item_ratings, brands, prices, prod_desc = load_data_hybrid(data_path, min_items=4, min_users=0, sampling= True, sample_size = 0.8)\n",
    "def transform_features (features):\n",
    "    if 'price' in features:\n",
    "        \"\"\"\n",
    "        prices = features['price']\n",
    "        prices_features= {}\n",
    "        prices_all = list(set(prices.values()))\n",
    "        price_quant_level = 10\n",
    "        price_max = float(max(prices.values()))\n",
    "        for key, value in prices.items():\n",
    "            prices_vec = numpy.zeros(price_quant_level+1)\n",
    "            idx = int(numpy.ceil(float(value)/(price_max/price_quant_level)))\n",
    "            prices_vec[idx]=1\n",
    "            prices_features[key] = prices_vec\n",
    "        features['price'] = prices_features\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        prices_log = {k:np.log(1+v) for k, v in prices.items()}\n",
    "        prices_log_features = {}\n",
    "        prices_log_all = list(set(prices_log.values()))\n",
    "        price_log_quantlevels = 10\n",
    "        price_log_max = float(max(prices_log.values()))\n",
    "        for key, value in prices_log.items():\n",
    "            prices_log_vec = numpy.zeros(price_log_quantlevels+1)\n",
    "            idx = int(numpy.ceil(float(value)/(price_log_max/price_log_quantlevels)))\n",
    "            prices_log_vec[idx]=1\n",
    "            prices_log_features[key] = prices_log_vec\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        prices = features['price']\n",
    "        prices_log = {k:np.log(1+v) for k, v in prices.items()}\n",
    "        prices_log_features = {}\n",
    "        #prices_log_all = list(set(prices_log.values()))\n",
    "        price_log_quantlevels = 10\n",
    "        price_log_max = float(max(prices_log.values()))\n",
    "        for key, value in prices_log.items():\n",
    "            prices_log_vec = numpy.zeros(price_log_quantlevels+1)\n",
    "            idx = int(numpy.ceil(float(value)/(price_log_max/price_log_quantlevels)))\n",
    "            prices_log_vec[idx]=1\n",
    "            prices_log_features[key] = prices_log_vec\n",
    "        features['price'] = prices_log_features\n",
    "        \n",
    "        \n",
    "        \n",
    "    for calc_feature in ['price_delta_calc1','price_delta_calc2','price_delta_l4avg']:\n",
    "        if calc_feature in features:\n",
    "            d_prices = features[calc_feature]\n",
    "            d_prices_log_features={}\n",
    "            d_price_min = min(d_prices.values())  \n",
    "            # Need to shift the value so that it is zero + 1 centered\n",
    "            d_prices_log = {k:np.log(1+abs(d_price_min) + v) for k, v in d_prices.items()}\n",
    "            #features['log_' + calc_feature] = d_prices_log\n",
    "            #d_prices_log_all = list(set(d_prices_log.values()))\n",
    "            d_price_log_quantlevels = 5\n",
    "            d_log_price_max = max(d_prices_log.values())\n",
    "            for key, value in d_prices_log.items():\n",
    "                d_log_prices_vec = numpy.zeros(d_price_log_quantlevels+1)\n",
    "                d_idx = int(numpy.ceil(float(value)/(d_log_price_max/d_price_log_quantlevels)))\n",
    "                d_log_prices_vec[d_idx]=1\n",
    "                d_prices_log_features[key] = d_log_prices_vec\n",
    "                #print (\"min:%f,max:%f\" % (price_min,price_max))\n",
    "                #print (\"index:%d,value:%f,shifted value:%f\" % (idx,value,shifted_value))\n",
    "            features[calc_feature] = d_prices_log_features\n",
    "    \n",
    "    \n",
    "    \n",
    "    if 'brand' in features:\n",
    "        brands_features = {}\n",
    "        brands = loaded_features['brand']\n",
    "        brands_all = list(set(brands.values()))\n",
    "        for key, value in brands.items():\n",
    "            brands_vec = numpy.zeros(len(brands_all))\n",
    "            brands_vec[brands_all.index(value)] = 1\n",
    "            brands_features[key] = brands_vec\n",
    "        features['brand'] = brands_features\n",
    "        \n",
    "    return features\n",
    "\n",
    "# list of features defined as dicts can be passed and they are combined, if none array of zeros are created\n",
    "\n",
    "def feature_set(feature_dicts=None):\n",
    "    if feature_dicts!=None:\n",
    "        combined_features = defaultdict(list)\n",
    "        for d in feature_dicts:\n",
    "            for k, v in d.items():  \n",
    "                combined_features[k].extend(v)\n",
    "\n",
    "        return dict([(k,v) for k,v in combined_features.items()])\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return {n: [0] for n in range(1,item_count+1)} #return just zeros dummy advanced features for baseline BPR\n",
    "\n",
    "def abpr(user_count, item_count, advanced_features, hidden_dim=10, hidden_adv_dim=10,\n",
    "         l2_regulization=0.1,\n",
    "         bias_regulization=0.01,\n",
    "         embed_regulization = 0,\n",
    "         adv_feature_regulization =0.1,\n",
    "         adv_feature_bias_regulization = 0.01):\n",
    "    \"\"\"\n",
    "    user_count: total number of users\n",
    "    item_count: total number of items\n",
    "    hidden_dim: hidden feature size of MF\n",
    "    hidden_adv_dim: hidden visual/non-visual feature size of MF\n",
    "    P.S. advanced_features can be one or many features combined. it can only be image features, non-image features, or both\n",
    "    \"\"\"\n",
    "    advanced_feat_dim = len(advanced_features[1])\n",
    "    iv = tf.placeholder(tf.float32, [None, advanced_feat_dim])\n",
    "    jv = tf.placeholder(tf.float32, [None, advanced_feat_dim])\n",
    "    u = tf.placeholder(tf.int32, [None])\n",
    "    i = tf.placeholder(tf.int32, [None])\n",
    "    j = tf.placeholder(tf.int32, [None])\n",
    "    \n",
    "    # model parameters -- LEARN THESE\n",
    "    # latent factors\n",
    "    user_emb_w = tf.get_variable(\"user_emb_w\", [user_count + 1, hidden_dim],\n",
    "                                 initializer=tf.random_normal_initializer(0, 0.1))\n",
    "    item_emb_w = tf.get_variable(\"item_emb_w\", [item_count + 1, hidden_dim],\n",
    "                                 initializer=tf.random_normal_initializer(0, 0.1))\n",
    "    # biases\n",
    "    item_b = tf.get_variable(\"item_b\", [item_count + 1, 1], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    # pull out the respective latent factor vectors for a given user u and items i & j\n",
    "    u_emb = tf.nn.embedding_lookup(user_emb_w, u)\n",
    "    i_emb = tf.nn.embedding_lookup(item_emb_w, i)\n",
    "    j_emb = tf.nn.embedding_lookup(item_emb_w, j)\n",
    "    \n",
    "    # get the respective biases for items i & j\n",
    "    i_b = tf.nn.embedding_lookup(item_b, i)\n",
    "    j_b = tf.nn.embedding_lookup(item_b, j)\n",
    "\n",
    "\n",
    "    # MF predict: u_i > u_j\n",
    "   \n",
    "    # UxD Advanced feature latent factors for users\n",
    "    user_adv_w = tf.get_variable(\"user_adv_w\", [user_count + 1, hidden_adv_dim],\n",
    "                             initializer=tf.random_normal_initializer(0, 0.1))\n",
    "    # this is E, the embedding matrix\n",
    "    item_adv_w = tf.get_variable(\"item_adv_w\", [hidden_adv_dim, advanced_feat_dim],\n",
    "                            initializer=tf.random_normal_initializer(0, 0.1))\n",
    "\n",
    "    theta_i = tf.matmul(iv, item_adv_w, transpose_b=True)  # (f_i * E), eq. 3\n",
    "    theta_j = tf.matmul(jv, item_adv_w, transpose_b=True)  # (f_j * E), eq. 3\n",
    "\n",
    "    adv_feature_bias = tf.get_variable(\"adv_feature_bias\", [1, advanced_feat_dim], initializer=tf.random_normal_initializer(0, 0.1))\n",
    "    # pull out the visual factor, 1 X D for user u\n",
    "\n",
    "    u_img = tf.nn.embedding_lookup(user_adv_w, u)\n",
    "\n",
    "    xui = i_b + tf.reduce_sum(tf.multiply(u_emb, i_emb), 1, keep_dims=True) + tf.reduce_sum(tf.multiply(u_img, theta_i), 1, keep_dims=True) \\\n",
    "                                                                        + tf.reduce_sum(tf.multiply(adv_feature_bias, iv), 1, keep_dims=True) \n",
    "    xuj = j_b + tf.reduce_sum(tf.multiply(u_emb, j_emb), 1, keep_dims=True) + tf.reduce_sum(tf.multiply(u_img, theta_j), 1, keep_dims=True) \\\n",
    "                                                                        + tf.reduce_sum(tf.multiply(adv_feature_bias, jv), 1, keep_dims=True) \n",
    "    l2_norm = tf.add_n([\n",
    "        l2_regulization * tf.reduce_sum(tf.multiply(u_emb, u_emb)),\n",
    "        adv_feature_regulization * tf.reduce_sum(tf.multiply(u_img, u_img)),\n",
    "        l2_regulization * tf.reduce_sum(tf.multiply(i_emb, i_emb)),\n",
    "        l2_regulization * tf.reduce_sum(tf.multiply(j_emb, j_emb)),\n",
    "        embed_regulization * tf.reduce_sum(tf.multiply(item_adv_w, item_adv_w)),\n",
    "        bias_regulization * tf.reduce_sum(tf.multiply(i_b, i_b)),\n",
    "        bias_regulization * tf.reduce_sum(tf.multiply(j_b, j_b)),\n",
    "        adv_feature_bias_regulization * tf.reduce_sum(tf.multiply(adv_feature_bias, adv_feature_bias))\n",
    "    ])\n",
    "        \n",
    "    xuij = xui - xuj\n",
    "\n",
    "    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "    \n",
    "    loss = l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(xuij)))\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    return xuij,u, i, j, iv, jv, loss, auc, train_op\n",
    "\n",
    "def session_run(num_iter, user_count, item_count, users, items, \n",
    "                user_ratings, item_ratings, advanced_features):\n",
    "    ### Loading and parsing the review matrix for Women 5-core dataset\n",
    "    auc_train = []\n",
    "    auc_test = []\n",
    "    auc_test_cs = []\n",
    "    #data_path = os.path.join('/Users/nolanthomas/Public/amazon', 'out_topcategories_pricepercentile_seasonmeteorological.csv')\n",
    "    #user_count, item_count, users, items, user_ratings, item_ratings, brands, features = load_data_hybrid(data_path, min_items=4, min_users=0, sampling= True, sample_size = 0.8)\n",
    "    user_ratings_test = generate_test(user_ratings)\n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "        with tf.variable_scope('abpr'):\n",
    "            xuij,u, i, j, iv, jv, loss, auc, train_op = abpr(user_count, item_count, advanced_features)\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "\n",
    "        for epoch in range(1, num_iter+1):\n",
    "            print (\"epoch \", epoch)\n",
    "            _loss_train = 0.0\n",
    "            user_count = 0\n",
    "            auc_train_values = []\n",
    "            for d, _iv, _jv in uniform_sample_batch(user_ratings, user_ratings_test, item_count, advanced_features):\n",
    "                user_count += 1\n",
    "                _loss, _auc, _ = session.run([loss, auc, train_op], feed_dict={u:d[:,0], i:d[:,1], j:d[:,2], iv:_iv, jv:_jv})\n",
    "                _loss_train += _loss\n",
    "                auc_train_values.append(_auc)\n",
    "            print (\"train_loss:\", _loss_train/user_count, \"train auc: \", numpy.mean(auc_train_values))\n",
    "            auc_train.append(numpy.mean(auc_train_values))\n",
    "\n",
    "            user_items_test=[]\n",
    "            auc_values = []\n",
    "            _loss_test = 0.0\n",
    "            user_count = 0\n",
    "            for d, _iv, _jv in test_batch_generator_by_user(user_ratings, user_ratings_test, item_ratings, item_count, advanced_features, cold_start = False):\n",
    "                user_count += 1\n",
    "                _loss, _auc = session.run([loss, auc], feed_dict={u: d[:, 0], i: d[:, 1], j: d[:, 2], iv: _iv, jv: _jv})\n",
    "                _loss_test += _loss\n",
    "                auc_values.append(_auc)\n",
    "            print (\"test_loss: \", _loss_test / user_count, \"test auc: \", numpy.mean(auc_values))\n",
    "            auc_test.append(numpy.mean(auc_values))\n",
    "\n",
    "            auc_values_cs = []\n",
    "            _loss_test_cs = 0.0\n",
    "            user_count = 0\n",
    "            for d, _iv, _jv in test_batch_generator_by_user(user_ratings, user_ratings_test, item_ratings, item_count, advanced_features, cold_start = True, cold_start_thresh = 10):\n",
    "                user_count += 1\n",
    "                _xuij,_loss, _auc = session.run([xuij,loss, auc], feed_dict={u: d[:, 0], i: d[:, 1], j: d[:, 2], iv: _iv, jv: _jv})\n",
    "                _loss_test_cs += _loss\n",
    "                auc_values_cs.append(_auc)\n",
    "                if epoch==num_iter:\n",
    "                    user_items_test.append((d,_xuij))\n",
    "            print (\"cold start test_loss: \", _loss_test_cs / user_count, \"cold start auc: \", numpy.mean(auc_values_cs))\n",
    "            auc_test_cs.append(numpy.mean(auc_values_cs))\n",
    "        return user_items_test,auc_train, auc_test, auc_test_cs\n",
    "\n",
    "def run(num_sessions, user_count, item_count, users, items, \n",
    "            user_ratings, item_ratings, advanced_features):\n",
    "    t1 = datetime.now()\n",
    "    user_items_test, auc_train, auc_test, auc_test_cold = session_run(num_sessions, user_count, item_count, \n",
    "                                                     users, items, user_ratings, item_ratings, \n",
    "                                                     advanced_features)\n",
    "    t2 = datetime.now()\n",
    "    return {'num_sessions':NUM_SESSIONS, 'sys.platform':str(sys.platform), \n",
    "            'sys.version':str(sys.version), \n",
    "            'user_items_test': user_items_test,\n",
    "            'auc_train': auc_train, 'auc_test': auc_test, \n",
    "            'auc_cold_test': auc_test_cold,\n",
    "            'start':format_time(t1),'end':format_time(t1),\n",
    "            'delta_sec':(t2-t1).total_seconds()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and make transformations; reuse across sessions\n",
    "#### 1. Load data\n",
    "#### 2. Make transformations\n",
    "#### 3. Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "bpr_data_file = os.path.join(home,'/Users/nirmal/Downloads/out_model_features.20180512.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_u_id:  34102\n",
      "max_i_id:  10303\n",
      "reviews :  131979\n",
      "u_id:  10218\n",
      "i_id:  10303\n",
      "reviews :  69211\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(bpr_data_file)\n",
    "user_count, item_count, users, items, user_ratings, item_ratings, loaded_features   = load_data_hybrid(data_path, min_items=4, min_users=0, sampling= True, sample_size = 0.8)\n",
    "# len(take(1, loaded_features['brand'].values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_features = transform_features(loaded_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running BASELINE BPR with no advanced features at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nvariants  = {\\n    'Price$':['price'],\\n    'Category-Tags, Subcat-L4, $Price':['top_categories','level4','price'],\\n    'Subcat-L4, $Price':['level4','price'],\\n    'Category-Tags, $Price':['top_categories','price'],\\n    'Category-Tags, Subcat-L4, Brand, $Price':['price','top_categories', 'level4', 'brand']\\n}\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define number of sessions or epochs\n",
    "NUM_SESSIONS = 3\n",
    "results = {}\n",
    "variants  = {\n",
    "#     'Category-Tags':['top_categories']#,\n",
    "#     'BPR':None,\n",
    "    #,\n",
    "    'Subcat-L4':['level4']\n",
    "    #'Brand':['brand'],\n",
    "    #'Price-L4-User':['price_delta_calc1'],\n",
    "    #'Price-4L-Avg':['price_delta_l4avg']\n",
    "}\n",
    "\"\"\"\n",
    "variants  = {\n",
    "    'Price$':['price'],\n",
    "    'Category-Tags, Subcat-L4, $Price':['top_categories','level4','price'],\n",
    "    'Subcat-L4, $Price':['level4','price'],\n",
    "    'Category-Tags, $Price':['top_categories','price'],\n",
    "    'Category-Tags, Subcat-L4, Brand, $Price':['price','top_categories', 'level4', 'brand']\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subcat-L4['level4']\n",
      "epoch  1\n",
      "train_loss: 0.79657267741 train auc:  0.670829\n",
      "test_loss:  2.16772876462 test auc:  0.580342\n",
      "cold start test_loss:  2.24769064637 cold start auc:  0.454155\n",
      "epoch  2\n",
      "train_loss: 0.691830861867 train auc:  0.719088\n",
      "test_loss:  1.68797983697 test auc:  0.609001\n",
      "cold start test_loss:  1.7293540574 cold start auc:  0.438135\n",
      "epoch  3\n",
      "train_loss: 0.631612273264 train auc:  0.754326\n",
      "test_loss:  1.56671794461 test auc:  0.641389\n",
      "cold start test_loss:  1.57222863976 cold start auc:  0.444649\n"
     ]
    }
   ],
   "source": [
    "# feature_set() is called without arguments\n",
    "for desc, features_to_use in variants.items():\n",
    "    print (desc + str(features_to_use))\n",
    "    if features_to_use != None:\n",
    "        features_list = feature_set([loaded_features[c] for c in features_to_use])\n",
    "    else:\n",
    "        features_list = feature_set()\n",
    "    results[desc] = run(NUM_SESSIONS, user_count, item_count, \n",
    "                         users, items, user_ratings, \n",
    "                         item_ratings,features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "datetime.now().strftime(\"%Y%m%d.%H%M\")\n",
    "pickle.dump( results, open( \"results.\"+datetime.now().strftime(\"%Y%m%d.%H%M\")\n",
    " + \".pickle\", \"wb\" ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Top 3 performers: 'Category-Tags, Subcat-L4, Brand'\n",
    "\n",
    "features_list = feature_set([loaded_features[c] for c in ['top_categories', 'level4', 'brand']])\n",
    "results['Category-Tags, Subcat-L4, Brand'] = run(NUM_SESSIONS, user_count, item_count, \n",
    "                         users, items, user_ratings, \n",
    "                         item_ratings,features_list)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "features_list = feature_set([loaded_features[c] for c in ['price']])\n",
    "results['Price$'] = run(NUM_SESSIONS, user_count, item_count, \n",
    "                         users, items, user_ratings, \n",
    "                         item_ratings,features_list)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparing Test AUC vs. number of iterations for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "datetime.now().strftime(\"%Y%m%d.%H%M\")\n",
    "pickle.dump( results, open( \"results.\"+datetime.now().strftime(\"%Y%m%d.%H%M\")\n",
    " + \".pickle\", \"wb\" ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "mpl.style.use('seaborn')\n",
    "\n",
    "\n",
    "def plot_auc_curve(results_to_graph, title, highlight):\n",
    "    dt_str = datetime.now().strftime(\"%Y%m%d.%H%M\")\n",
    "    sns.set_context(\"talk\")\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    plt.title(title,fontsize=30)\n",
    "    for calc_desc, calc_results in results_to_graph.items():\n",
    "        ls='solid'\n",
    "        lw=3\n",
    "        ms=7\n",
    "        if calc_desc == 'BPR':\n",
    "            lw=5\n",
    "            ms=11\n",
    "            ls='dashed'\n",
    "        if calc_desc == highlight:\n",
    "            lw=5\n",
    "            ms=11\n",
    "            ls='-.'\n",
    "        plt.plot(calc_results['auc_test'], \n",
    "            label=calc_desc,\n",
    "            linewidth=lw,\n",
    "            linestyle=ls,\n",
    "            markersize=ms,\n",
    "            marker='o')\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Test AUC\",fontsize=20)\n",
    "    plt.xlabel(\"Number of Iterations\",fontsize=20)\n",
    "    #savefig('auc_curve.' + dt_str + '.png')\n",
    "    #show()\n",
    "\n",
    "def plot_auc_cold_start_curve(results_to_graph, title,highlight): \n",
    "    dt_str = datetime.now().strftime(\"%Y%m%d.%H%M\")\n",
    "    sns.set_context(\"talk\")\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    plt.title(title,fontsize=30)\n",
    "    for calc_desc, calc_results in results_to_graph.items():\n",
    "        ls='solid'\n",
    "        lw=3\n",
    "        ms=7\n",
    "        if calc_desc == 'BPR':\n",
    "            lw=5\n",
    "            ms=11\n",
    "            ls='dashed'\n",
    "        if calc_desc == highlight:\n",
    "            lw=5\n",
    "            ms=11\n",
    "            ls='-.'\n",
    "        plt.plot(calc_results['auc_cold_test'],\n",
    "            label=calc_desc,\n",
    "            linewidth=lw,\n",
    "            linestyle=ls,\n",
    "            markersize=ms,\n",
    "            marker='o')\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Cold Start Test AUC\",fontsize=20)\n",
    "    plt.xlabel(\"Number of Iterations\",fontsize=20)\n",
    "    #plt.savefig('auc_cold_start_curve.' + dt_str + '.png')\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualize_list = ['BPR', 'Category-Tags','Subcat-L4']\n",
    "results_to_graph ={v:results[v] for v in visualize_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rpt_date = datetime.now().strftime(\"%Y%m%d.%H%M\")\n",
    "plot_auc_curve(results,'BPR Features Test AUC - ' + rpt_date ,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_auc_cold_start_curve(results, 'BPR AFeatures Cold Start AUC - ' + rpt_date,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_feature = ['BPR', 'Category-Tags', 'Subcat-L4', 'Brand', 'Price$']\n",
    "results_to_graph ={v:results[v] for v in single_feature}\n",
    "plot_auc_curve(results_to_graph,'BPR Features Test AUC','Category-Tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_auc_cold_start_curve(results_to_graph,\n",
    "                          'BPR Features Cold Start Test AUC',\n",
    "                          'Category-Tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combos = ['Category-Tags, Subcat-L4, $Price', 'Subcat-L4, $Price', 'Category-Tags, $Price', 'Category-Tags, Subcat-L4, Brand, $Price', 'BPR', 'Category-Tags']\n",
    "results_to_graph ={v:results[v] for v in combos}\n",
    "plot_auc_curve(results_to_graph,\n",
    "               'BPR Test AUC\\nCategory Tags vs Feature Combinations',\n",
    "               'Category-Tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_auc_cold_start_curve(results_to_graph,\n",
    "                          'BPR Cold Start Test AUC\\nCategory Tags vs Feature Combinations',\n",
    "                          'Category-Tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bpr_data_file = !ls ~/data/out_model_features.csv\n",
    "bpr_data_file[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "results_0141 = pickle.load( open( \"results.20180511.0141.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = [v['delta_sec'] for k,v in results.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_str = datetime.now().strftime(\"%Y%m%d.%H%M\")\n",
    "sns.set_context(\"talk\")\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "#plt.title(title,fontsize=30)\n",
    "plt.scatter(z,\n",
    "    #label=calc_desc,\n",
    "    #linewidth=lw,\n",
    "    #linestyle=ls,\n",
    "    #markersize=ms,\n",
    "    marker='o')\n",
    "plt.legend()\n",
    "#plt.ylabel(\"Cold Start Test AUC\",fontsize=20)\n",
    "#plt.xlabel(\"Number of Iterations\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k,v in results.items():   \n",
    "    print('AUC Test -' + k +  str(v['auc_test'][-1]))\n",
    "    print('Cold Start AUC Test - '+ k + str(v['auc_cold_test'][-1]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR NOLAN TO ADD\n",
    "f = {}\n",
    "\n",
    "for i in (x[0]for x in enumerate(user_items_test)):\n",
    "    for x in zip(user_items_test[i][0], user_items_test[i][1]):\n",
    "        f[(x[0][0], x[0][1], x[0][2])] = x[1][0]\n",
    "\n",
    "user_item_rec_score = {}\n",
    "user_item_pur_score={}\n",
    "\n",
    "for x in list(f.keys()):\n",
    "    \n",
    "    # if SCORE is less than zero meaning USER preffered j Over i\n",
    "    if f[x] < 0:\n",
    "        if x[0] not in user_item_rec_score.keys():\n",
    "            user_item_rec_score[x[0]] = []\n",
    "        \n",
    "        user_item_rec_score[x[0]].append((x[2], f[x]))\n",
    "    \n",
    "    # SCORE of more than 0, meaning user preffered i over j\n",
    "    else:\n",
    "       \n",
    "        if x[0] not in user_item_pur_score.keys():\n",
    "            user_item_pur_score[x[0]] = []\n",
    "        \n",
    "        user_item_pur_score[x[0]].append((x[1], f[x]))\n",
    "\n",
    "#reverse dict for look up\n",
    "users_lookup= {v:k for k,v in users.items()}\n",
    "items_lookup= {v:k for k,v in items.items()}\n",
    "\n",
    "array=[]\n",
    "for x in list(user_item_rec_score.keys()):\n",
    "    a=user_item_rec_score[x]\n",
    "    b= sorted(a, key=lambda x: x[1])\n",
    "    for i in range(10): #recommending top 10 items\n",
    "        array.append((users_lookup[x], items_lookup[b[i][0]]))  \n",
    "        \n",
    "        \n",
    "#create dataframe of user and their REC LIST- Top 10 SORTED by high to low Preference score so ORDER matters\n",
    "df=pd.DataFrame(array, columns=['User','Recommended Items'])\n",
    "df_Rec=df.groupby('User', as_index=False)['Recommended Items'].agg({'Recommendation list':(lambda x: list(x))})\n",
    "df_Rec.head()\n",
    "\n",
    "c=df_Rec.set_index('User').T.to_json()\n",
    "\n",
    "import json as json\n",
    "with open('reclist_json', 'w+') as f: \n",
    "    f.write(json.dumps(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
